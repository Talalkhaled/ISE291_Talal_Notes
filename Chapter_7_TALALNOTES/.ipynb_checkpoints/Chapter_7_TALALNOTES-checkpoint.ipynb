{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c23c51",
   "metadata": {},
   "source": [
    "# Talal Notes for chapter 7 ISE291\n",
    "\n",
    "<mark> **These notes are not to undermine the importance of the course slides, but rather provide extra help for understanding and applying code and explain the concept in a simplified manner!**</mark>\n",
    "    \n",
    "<span style = 'color:#ec042d'> **Please review the course material for a more detailed understanding** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80feba",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Predict the value of a dependent variable based on the value of at least one independent variable\n",
    "- Explain the impact of changes in an independent variable on the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a57fb",
   "metadata": {},
   "source": [
    "## In Single Linear Regression (SLR) or (Single Input Single Output SISO)\n",
    "\n",
    "- Only one independent variable, X\n",
    "- Relationship between X and Y is described by a linear function\n",
    "- Changes in Y are assumed to be related to changes in X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc99b96",
   "metadata": {},
   "source": [
    "<img src=\"img/ISE291_CH7_IMG1.png\" width=440 height=110/>\n",
    "<img src=\"img/ISE291_CH7_IMG2.png\" width=440 height=110/>\n",
    "<img src=\"img/ISE291_CH7_IMG3.png\" width=440 height=110/>\n",
    "<img src=\"img/ISE291_CH7_IMG4.png\" width=440 height=110/>\n",
    "\n",
    "$$ \\beta_1 = r \\frac{sd_y}{sd_x} $$ and $$ \\beta_0 = \\overline{y} - \\beta_1 \\overline{x} $$\n",
    "where $r$ is the Pearson's correlation coefficient, $sd_x$ and $sd_y$ represent the standard deviation of $x$ and $y$ variables respectively, $\\overline{x}$ and $\\overline{y}$ represent the means of $x$ and $y$ variables respectively.\n",
    "\n",
    "### <mark>Please remember that in chapter 6 there is a video that explained how to get the correlation coefficient.</mark>\n",
    "\n",
    "At the exact same moment we can see ùõΩ1 and ùõΩ0\n",
    "\n",
    "From this video @ [2:21], https://www.youtube.com/watch?v=ttTO7CetjVk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0121a960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the relationship between Attitude and Score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('data/Regression-1.csv', delimiter =',')\n",
    "plt.figure()\n",
    "sns.relplot(x='Attitude', y='Score',\n",
    "            color = '#e06cba', marker = 'D',\n",
    "            kind='scatter',\n",
    "            data=df)\n",
    "plt.xlabel('Attitude')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11d226",
   "metadata": {},
   "source": [
    "### <mark>There exists a linear relation between the two variables and it seems to also be a strong linear relation</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b11fe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attitude</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attitude  Score\n",
       "0        65    129\n",
       "1        67    126\n",
       "2        68    143\n",
       "3        70    156\n",
       "4        71    161\n",
       "5        72    158\n",
       "6        72    168\n",
       "7        73    166\n",
       "8        73    182\n",
       "9        75    201"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.iloc[:,1:]) ## Use your calculator to get our coefficients (r, ùõΩ1, and ùõΩ0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1d4c3",
   "metadata": {},
   "source": [
    "### <mark>Using \"from sklearn.linear_model import LinearRegression\", get the needed coefficients</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9a24d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta1, the slope is 6.93; and Beta0, the intercept is -330.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "reg = LinearRegression(fit_intercept=True)\n",
    "reg.fit(df[['Attitude']], df['Score'])\n",
    "\n",
    "print(f'Beta1, the slope is {np.round(reg.coef_,2)[0]}; and Beta0, the intercept is {np.round(reg.intercept_,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be843f",
   "metadata": {},
   "source": [
    "## <mark>This type of regression is called Ordinary Least Square (OLS) regression.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc025c4",
   "metadata": {},
   "source": [
    "## Our prediction model should look like this\n",
    "$$\\hat{y} =-330.46 + 6.93 {x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6430bb",
   "metadata": {},
   "source": [
    "## Now we can estimate/predict the output for values that are not in our data\n",
    "\n",
    "Question: Predict the score of an attitude of 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f767d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224.17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talal\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(np.round(reg.predict(np.array(80).reshape(1, -1)),2)) #single value or single record, we need the .reshape() method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed2b0791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated scores for the students with attiudes of [78, 74, 68, 69] are [210.3, 182.57, 140.97, 147.91] respectively .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talal\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### For multible predictions of x\n",
    "\n",
    "x_new = [78,74,68,69] ## Define a list of your x values that you want to predict the output for\n",
    "y_new = reg.predict(np.array(x_new).reshape(-1, 1))  #single column\n",
    "print(f'The estimated scores for the students with attiudes of {x_new} are {np.round(y_new,2).tolist()} respectively .')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b0d24",
   "metadata": {},
   "source": [
    "# Gradient Descent for Single Linear Regression\n",
    "\n",
    "- Gradient Descent is an optimization algorithm that tries to find the best parameters that minimize the model's cost function (error function)\n",
    "    - for a given value of ùõΩ1 and ùõΩ0, the error function returns an error value based on how well the line fits our data. \n",
    "- Please focus on the differences between the cost function that is used in our Gradient Descent algorithm, and the way Gradient Descent is formulated.\n",
    "\n",
    "- Cost function >> $$ \\epsilon = \\frac{1}{n} \\sum_{i=1}^n ((\\beta_1x_i +\\beta_0)-y_i)^2.$$\n",
    "\n",
    "- We need to use partial derivatives w.r.t $\\beta_1$ and $\\beta_0$ that are:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\epsilon}{\\partial \\beta_1} = \\frac{2}{n}\\sum_{i=1}^n ((\\beta_1x_i +\\beta_0)-y_i)x_i,\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\epsilon}{\\partial \\beta_0} = \\frac{2}{n}\\sum_{i=1}^n ((\\beta_1x_i +\\beta_0)-y_i).\n",
    "$$\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "\n",
    "$\\quad$ Initialize:  \n",
    "$\\quad$$\\quad$ Start with **random** $\\beta_1$ and $\\beta_0$ values, say $\\beta_1^{old}$ and $\\beta_0^{old}$.  \n",
    "$\\quad$$\\quad$ Calculate $\\epsilon^{old} (error)$  \n",
    "$\\quad$ repeat {  \n",
    "$\\quad$$\\quad$ Update:  \n",
    "$\\quad$$\\quad$$\\quad$ $ \\beta_1^{new} = \\beta_1^{old} - \\lambda \\frac{2}{n}\\sum_{i=1}^n ((\\beta_1^{old} x_i +\\beta_0^{old})-y_i)x_i$   <span style = 'color:#ec042d'>**Notice the partial derivative term that is used here that has been defined above**</span>\n",
    "\n",
    "$\\quad$$\\quad$$\\quad$ $ \\beta_0^{new} = \\beta_0^{old} - \\lambda \\frac{2}{n}\\sum_{i=1}^n ((\\beta_1^{old} x_i +\\beta_0^{old})-y_i)$  \n",
    "$\\quad$$\\quad$$\\quad$ Calculate $\\epsilon^{new}$  \n",
    "$\\quad$ } until ($\\epsilon^{new}$ >= $\\epsilon^{old}$)\n",
    "\n",
    "In the above algorithm $\\lambda>0$ is step size, or sometimes called the learning rate, and is usually selected as a small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c62e1a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Values: 10 10 8210.8 \n",
      "\n",
      "Final values: -10.263152927431642 3.977443244976516 32.53308270677262\n"
     ]
    }
   ],
   "source": [
    "# Implementation of gradient descent method.\n",
    "\n",
    "def error_function(x,y,b0,b1):\n",
    "    e=0\n",
    "    for i,yi in enumerate(y): ## Since we iterate over the values of y we don't need to use y[i], although we can use it\n",
    "        e += ((b1*x[i]+b0) - yi)**2\n",
    "    return e/len(y)\n",
    "\n",
    "def partial_b1(x,y,b0,b1):\n",
    "    e=0\n",
    "    for i,xi in enumerate(x): ## We iterate this time over values of x, so now we use y[i], this note is to not confuse about yi\n",
    "        e += ((b1*xi+b0) - y[i])*xi\n",
    "    return 2*e/len(y)\n",
    "\n",
    "def partial_b0(x,y,b0,b1):\n",
    "    e=0\n",
    "    for i,yi in enumerate(y):\n",
    "        e += ((b1*x[i]+b0) - yi)\n",
    "    return 2*e/len(y)\n",
    "\n",
    "# Given\n",
    "\n",
    "df = pd.read_csv('data/Regression-2.csv', delimiter =',')\n",
    "x = df['x'].values\n",
    "y = df['y'].values\n",
    "\n",
    "#initialize\n",
    "b1_Old = 10\n",
    "b0_Old = 10\n",
    "mse_Old = error_function(x,y,b0_Old,b1_Old) \n",
    "lam = 0.005\n",
    "print('Initial Values:', b0_Old, b1_Old, mse_Old,'\\n')\n",
    "##repeate\n",
    "i = 0\n",
    "while True:\n",
    "    i+=1\n",
    "    b1_New = b1_Old - lam * partial_b1(x,y,b0_Old,b1_Old) ## (2/n) * the summation term is defined in the functions above\n",
    "    b0_New = b0_Old - lam * partial_b0(x,y,b0_Old,b1_Old)\n",
    "    mse_New = error_function(x,y,b0_New,b1_New)\n",
    "    if mse_New >= mse_Old:\n",
    "        b0_New,b1_New,mse_New = b0_Old,b1_Old,mse_Old\n",
    "        break\n",
    "    else:\n",
    "        b0_Old,b1_Old,mse_Old = b0_New,b1_New,mse_New  \n",
    "      \n",
    "print('Final values:', b0_New, b1_New, mse_New)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996e8bc",
   "metadata": {},
   "source": [
    "### <mark>Here are some easy steps to follow to understand gradient descent</mark>\n",
    "\n",
    "- (1) Initialize values for  $\\beta_1$ and $\\beta_0$\n",
    "- (2) Calculate the error function using the initialized  $\\beta_1$ and $\\beta_0$\n",
    "- (3) In the loop, we update values of B0 and B1. \n",
    "    - we then calculate a new error function.\n",
    "    - If our **NEW** error function is less than our **OLD** error function, this means we are going into the right direction of minimizing our error function, we **continue** our loop.\n",
    "    - Our new values become our old values. This is done to calculate the newer values.\n",
    "    - If our **OLD** error function is less than our **NEW** function, this means we need to stop our loop, minimization is not guaranteed.\n",
    "    - We want to bring back our values that had the minimum error, so we assign our **NEW** values to our **OLD** values that had the minimum function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b11f7",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression (MLR) or (Multiple Input Single Output MISO)\n",
    "\n",
    "In this chapter, for MLR, we are only considered with the following:\n",
    "\n",
    "- Closed Form Solution for MISO Linear Regression (parameters of MLR)\n",
    "- How to get parameters using python libraries\n",
    "- The definition of gradient descent for MLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc472b4",
   "metadata": {},
   "source": [
    "The closed form looks like this>> $$(\\mathbf{X^{T}X})^{-1} \\mathbf{X^{T}y}$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{n \\times (P+1)}$, each column of $\\mathbf{X}$ represent an input variable, the first column of $\\mathbf{X}$ contains all ones, and $\\mathbf{y} \\in  \\mathbb{R}^{n}$ represents the column containing output variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc4a9424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.645861</td>\n",
       "      <td>-2.016634</td>\n",
       "      <td>-1.872101</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>-0.525657</td>\n",
       "      <td>-0.867655</td>\n",
       "      <td>-1.047571</td>\n",
       "      <td>-0.868957</td>\n",
       "      <td>-2.533318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.999313</td>\n",
       "      <td>-0.725759</td>\n",
       "      <td>-0.791989</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>-0.525657</td>\n",
       "      <td>-0.867655</td>\n",
       "      <td>-1.047571</td>\n",
       "      <td>-0.868957</td>\n",
       "      <td>-2.299712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.587021</td>\n",
       "      <td>-2.200154</td>\n",
       "      <td>1.368234</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>-0.525657</td>\n",
       "      <td>-0.867655</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>-0.156155</td>\n",
       "      <td>-2.299712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.178174</td>\n",
       "      <td>-0.812191</td>\n",
       "      <td>-0.791989</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>-0.525657</td>\n",
       "      <td>-0.867655</td>\n",
       "      <td>-1.047571</td>\n",
       "      <td>-0.868957</td>\n",
       "      <td>-2.299712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.510513</td>\n",
       "      <td>-0.461218</td>\n",
       "      <td>-0.251933</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>-0.525657</td>\n",
       "      <td>-0.867655</td>\n",
       "      <td>-1.047571</td>\n",
       "      <td>-0.868957</td>\n",
       "      <td>-1.834631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.262444</td>\n",
       "      <td>0.580608</td>\n",
       "      <td>0.558151</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>1.902379</td>\n",
       "      <td>1.079149</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>1.269449</td>\n",
       "      <td>1.660415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2.107397</td>\n",
       "      <td>0.628738</td>\n",
       "      <td>-2.682185</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>1.902379</td>\n",
       "      <td>1.688267</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>0.556647</td>\n",
       "      <td>1.921044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.328267</td>\n",
       "      <td>-0.546127</td>\n",
       "      <td>-1.602073</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>1.902379</td>\n",
       "      <td>1.900197</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>-0.512556</td>\n",
       "      <td>2.320465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.307045</td>\n",
       "      <td>0.340141</td>\n",
       "      <td>0.558151</td>\n",
       "      <td>1.010033</td>\n",
       "      <td>1.902379</td>\n",
       "      <td>1.249088</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>1.982251</td>\n",
       "      <td>2.611649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.809719</td>\n",
       "      <td>0.811961</td>\n",
       "      <td>0.558151</td>\n",
       "      <td>0.234114</td>\n",
       "      <td>1.902379</td>\n",
       "      <td>2.216735</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>-0.156155</td>\n",
       "      <td>2.703452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  -1.645861 -2.016634 -1.872101 -1.030029 -0.525657 -0.867655 -1.047571   \n",
       "1  -1.999313 -0.725759 -0.791989 -1.030029 -0.525657 -0.867655 -1.047571   \n",
       "2  -1.587021 -2.200154  1.368234 -1.030029 -0.525657 -0.867655  0.344407   \n",
       "3  -2.178174 -0.812191 -0.791989 -1.030029 -0.525657 -0.867655 -1.047571   \n",
       "4  -0.510513 -0.461218 -0.251933 -1.030029 -0.525657 -0.867655 -1.047571   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "92  1.262444  0.580608  0.558151 -1.030029  1.902379  1.079149  0.344407   \n",
       "93  2.107397  0.628738 -2.682185 -1.030029  1.902379  1.688267  0.344407   \n",
       "94  1.328267 -0.546127 -1.602073 -1.030029  1.902379  1.900197  0.344407   \n",
       "95  1.307045  0.340141  0.558151  1.010033  1.902379  1.249088  0.344407   \n",
       "96  1.809719  0.811961  0.558151  0.234114  1.902379  2.216735  0.344407   \n",
       "\n",
       "          x8         y  \n",
       "0  -0.868957 -2.533318  \n",
       "1  -0.868957 -2.299712  \n",
       "2  -0.156155 -2.299712  \n",
       "3  -0.868957 -2.299712  \n",
       "4  -0.868957 -1.834631  \n",
       "..       ...       ...  \n",
       "92  1.269449  1.660415  \n",
       "93  0.556647  1.921044  \n",
       "94 -0.512556  2.320465  \n",
       "95  1.982251  2.611649  \n",
       "96 -0.156155  2.703452  \n",
       "\n",
       "[97 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Regression-3.csv', delimiter =',')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f44988",
   "metadata": {},
   "source": [
    "### We can see clearly our output column here, \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1f24c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.280521</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.027350</td>\n",
       "      <td>0.538845</td>\n",
       "      <td>0.675310</td>\n",
       "      <td>0.432417</td>\n",
       "      <td>0.433652</td>\n",
       "      <td>0.734460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>0.280521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347969</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>0.155385</td>\n",
       "      <td>0.164537</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>0.107354</td>\n",
       "      <td>0.433319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.347969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.350186</td>\n",
       "      <td>0.117658</td>\n",
       "      <td>0.127668</td>\n",
       "      <td>0.268892</td>\n",
       "      <td>0.276112</td>\n",
       "      <td>0.169593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>0.027350</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>0.350186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.085843</td>\n",
       "      <td>-0.006999</td>\n",
       "      <td>0.077820</td>\n",
       "      <td>0.078460</td>\n",
       "      <td>0.179809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5</th>\n",
       "      <td>0.538845</td>\n",
       "      <td>0.155385</td>\n",
       "      <td>0.117658</td>\n",
       "      <td>-0.085843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.673111</td>\n",
       "      <td>0.320412</td>\n",
       "      <td>0.457648</td>\n",
       "      <td>0.566218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x6</th>\n",
       "      <td>0.675310</td>\n",
       "      <td>0.164537</td>\n",
       "      <td>0.127668</td>\n",
       "      <td>-0.006999</td>\n",
       "      <td>0.673111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.514830</td>\n",
       "      <td>0.631528</td>\n",
       "      <td>0.548813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x7</th>\n",
       "      <td>0.432417</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>0.268892</td>\n",
       "      <td>0.077820</td>\n",
       "      <td>0.320412</td>\n",
       "      <td>0.514830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751905</td>\n",
       "      <td>0.368987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x8</th>\n",
       "      <td>0.433652</td>\n",
       "      <td>0.107354</td>\n",
       "      <td>0.276112</td>\n",
       "      <td>0.078460</td>\n",
       "      <td>0.457648</td>\n",
       "      <td>0.631528</td>\n",
       "      <td>0.751905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.422316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.734460</td>\n",
       "      <td>0.433319</td>\n",
       "      <td>0.169593</td>\n",
       "      <td>0.179809</td>\n",
       "      <td>0.566218</td>\n",
       "      <td>0.548813</td>\n",
       "      <td>0.368987</td>\n",
       "      <td>0.422316</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2        x3        x4        x5        x6        x7  \\\n",
       "x1  1.000000  0.280521  0.225000  0.027350  0.538845  0.675310  0.432417   \n",
       "x2  0.280521  1.000000  0.347969  0.442264  0.155385  0.164537  0.056882   \n",
       "x3  0.225000  0.347969  1.000000  0.350186  0.117658  0.127668  0.268892   \n",
       "x4  0.027350  0.442264  0.350186  1.000000 -0.085843 -0.006999  0.077820   \n",
       "x5  0.538845  0.155385  0.117658 -0.085843  1.000000  0.673111  0.320412   \n",
       "x6  0.675310  0.164537  0.127668 -0.006999  0.673111  1.000000  0.514830   \n",
       "x7  0.432417  0.056882  0.268892  0.077820  0.320412  0.514830  1.000000   \n",
       "x8  0.433652  0.107354  0.276112  0.078460  0.457648  0.631528  0.751905   \n",
       "y   0.734460  0.433319  0.169593  0.179809  0.566218  0.548813  0.368987   \n",
       "\n",
       "          x8         y  \n",
       "x1  0.433652  0.734460  \n",
       "x2  0.107354  0.433319  \n",
       "x3  0.276112  0.169593  \n",
       "x4  0.078460  0.179809  \n",
       "x5  0.457648  0.566218  \n",
       "x6  0.631528  0.548813  \n",
       "x7  0.751905  0.368987  \n",
       "x8  1.000000  0.422316  \n",
       "y   0.422316  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.corr()) ## Notice the diagonal of 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccc0ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closed form estimates are: [-0.0, 0.58, 0.23, -0.14, 0.12, 0.27, -0.13, 0.03, 0.11]\n"
     ]
    }
   ],
   "source": [
    "## Implementation of closed form\n",
    "\n",
    "Xo = df.iloc[:,0:-1].values ## column -1 is not inculded, therefore we took only our x columns\n",
    "y = df.iloc[:,-1].values\n",
    "X = np.c_[np.ones(len(df.index)), Xo] \n",
    "## Remember that the X we want contains all ones in its first column, this is how we add it\n",
    "\n",
    "best_beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "print('The closed form estimates are:', np.round(best_beta,2).tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb8074c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best values for the estimates are : -0.0 [0.58, 0.23, -0.14, 0.12, 0.27, -0.13, 0.03, 0.11]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coefficient estimates using scikit learn LinearRegression module. \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(Xo, y)\n",
    "\n",
    "best_beta =  np.round(reg.coef_,2)\n",
    "best_beta_0 = np.round(reg.intercept_,2)\n",
    "\n",
    "print(f'The best values for the estimates are :', best_beta_0, best_beta.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb81fb",
   "metadata": {},
   "source": [
    "# Penalized Linear Regression (PLR)\n",
    "\n",
    "### The following notes are all from course notes, please read them carefully, and focus on the differences between the Ridge and Lasso regression >> $$\\beta_j^2 and |\\beta_j|$$\n",
    "- The coefficients obtained by minimizing the squared error of the training data, may not perform in general good on the new data.\n",
    "- This phenomenon is called as **over-fitting**.\n",
    "- In order to control over-fitting, a penalized term (or regularization term) is added to the _error function_. \n",
    "- The aim of the penalized function is to set some of the coefficients to zero.\n",
    "\n",
    "- In PLR, the least square error function will be updated as follows:\n",
    " $$J(\\boldsymbol \\beta) = \\frac{1}{n} \\sum_{i=1}^n\\left( h(x^i)-y_i \\right)^2 + \\alpha P(\\boldsymbol \\beta),$$\n",
    " \n",
    " where $P(\\boldsymbol \\beta)$ is the penalty or regularized term, and $\\alpha$ is the regularization coefficient.\n",
    "- In our class, we will look at the following two popular penalization linear regression models:\n",
    " - Ridge regression:\n",
    " $$\\frac{1}{n} \\sum_{i=1}^n\\left( h(x^i)-y_i \\right)^2 + \\alpha \\sum_{j=0}^P \\beta_j^2$$\n",
    " - LASSO regression:\n",
    "  $$\\frac{1}{n} \\sum_{i=1}^n\\left( h(x^i)-y_i \\right)^2 + \\alpha \\sum_{j=0}^P |\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292030c",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "<span style = 'color:#038eff'>\n",
    "    \n",
    "In **Ridge** regression a closed form solution exists, given as ùú∑‚àó=(ùêóùêìùêó+ùõºùêà)‚àí1ùêóùêìùê≤\n",
    "\n",
    "In **Lasso**, no closed form solution, but many efficient algorithms exists.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8b88741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best values for the estimates are : -0.0 [0.1, 0.06, 0.01, 0.02, 0.07, 0.06, 0.04, 0.04]\n"
     ]
    }
   ],
   "source": [
    "# Find the coefficient estimates using Ridge regression for  ùëéùëôùëù‚Ñéùëé=500 , using sci-kit learn.\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/Regression-3.csv', delimiter =',')\n",
    "\n",
    "Xo = df.iloc[:,0:-1].values\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "regr = Ridge(alpha=500)\n",
    "regr.fit(Xo, y)\n",
    "best_beta =  np.round(regr.coef_,2)\n",
    "best_beta_0 = np.round(regr.intercept_,2)\n",
    "print(f'The best values for the estimates are :', best_beta_0, best_beta.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebf6fad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best values for the estimates are : -0.0 [0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Find the coefficient estimates using Lasso regression for  ùëéùëôùëù‚Ñéùëé=0.5 , using sci-kit learn.\n",
    "from sklearn.linear_model import Lasso\n",
    "regl = Lasso(alpha=0.5)\n",
    "regl.fit(Xo, y)\n",
    "best_beta =  np.round(regl.coef_,2)\n",
    "best_beta_0 = np.round(regl.intercept_,2)\n",
    "print(f'The best values for the estimates are :', best_beta_0, best_beta.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d41666",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- OLS vs Ridge: OLS and Ridge have all non-zero coefficients. However, the coefficients given by Ridge are less in terms of absolute value (shrunken coefficients)\n",
    "\n",
    "- OLS vs Lasso: OLS has all non-zero coefficients, whereas Lasso has ONE non-zero coefficients (selected coefficient). Thus, Lasso claims that only the first column is relevant to the output variable, for alpha=0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd70a32",
   "metadata": {},
   "source": [
    "### Lasso vs Ridge: Summary\n",
    "\n",
    "### Important to read!\n",
    "\n",
    "- Ridge regression shrinks the estimate values, it is useful when there are **correlated input columns**.\n",
    "- Lasso does feature selection, it is useful when there are **unrelated input columns**.\n",
    "- The quality of solution depends on the value of the hyper-parameter $\\alpha$\n",
    "- Selecting the best value of $\\alpha$ generates better results than OLS! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11fe0c4",
   "metadata": {},
   "source": [
    "# Train-Testing vs Cross-validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766d3c0",
   "metadata": {},
   "source": [
    "**Train-Testing:**\n",
    "- The aim is to test the model's __generalizability__.\n",
    " - to goal is to reduce the level of overfitting.\n",
    " - the training is done a little bit more rigorously.\n",
    " - validation set is for validating the model's accuracy before the actual test on testing data.\n",
    " \n",
    "**Cross-validation (CV):**\n",
    "- It is used when there is no predefined training/validation/testing data.\n",
    "- The idea is to partition the data into training/validation/testing sets.\n",
    "- The aim is to improve the model's __generalizability__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e0729",
   "metadata": {},
   "source": [
    "## <mark> More on Train-Testing & Cross-validation (CV)</mark>\n",
    "\n",
    "### Please read!\n",
    "\n",
    "We train (fit) on the **Training** data, and we predict on a separate **Testing** data\n",
    "\n",
    "We use **Train** data **AND** **Cross-Validation** data to get the best value alpha ($\\alpha$)\n",
    "\n",
    "*Differences between Test Train-Testing & Cross-validation are*:\n",
    "\n",
    "- Train-Testing  $\\rightarrow$ The testing and training data are already given\n",
    "- CV $\\rightarrow$ We **randomly** generate the training and testing data from a single given dataframe(*DF*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b17a4",
   "metadata": {},
   "source": [
    "### Please go to course slides for the case study.\n",
    "\n",
    "### Here are some notes about it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50557de4",
   "metadata": {},
   "source": [
    "X = df.iloc[:,:-1].values\n",
    "\n",
    "y = df.iloc[:, -1].values \n",
    "\n",
    "X_train, X_test, y_train, y_test = <span style = 'color:#ec042d'>train_test_split(X, y, test_size=0.3, random_state=42)</span>\n",
    "\n",
    "-\n",
    "- We usually split the data into 30% for testing and 70% for training, or 25% and 75%. Make sure the training has the larger split\n",
    "- Controls the shuffling applied to the data before applying the split, Keep the random state at 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb7ded",
   "metadata": {},
   "source": [
    "### After we have our training and testing splits, we need to scale/normalize them\n",
    "\n",
    "scaler.fit(np.c_[X_train,y_train])\n",
    "\n",
    "A_train = scaler.transform(np.c_[X_train,y_train])\n",
    "\n",
    "X_train = A_train[:,:-1]\n",
    "\n",
    "y_train = A_train[:,-1]\n",
    "\n",
    "A_test = scaler.transform(np.c_[X_test,y_test])\n",
    "\n",
    "X_test = A_test[:,:-1]\n",
    "\n",
    "y_test = A_test[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff6782",
   "metadata": {},
   "source": [
    "### ‚ÄúSuccess is no accident. It is hard work, perseverance, learning, studying, sacrifice, and most of all, love of what you are doing or learning to do.‚Äù ‚Äï Pele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20007976",
   "metadata": {},
   "source": [
    "### For any questions please contact me:\n",
    "\n",
    "E-mail: 202036240@kfupm.edu.sa\n",
    "\n",
    "LinkedIn: https://www.linkedin.com/in/talal-harbi\n",
    "\n",
    "Twitter: TalalkhaledHr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d2bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
